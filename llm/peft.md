# LLM Parameter Efficient Fine-Tuning (PEFT)

Maintainers - [Aaron Zhao](https://aaron-zhao123.github.io/) and Zixi Zhang

A curated list of LLM PEFT papers, partially taken from Sudarsharm
Sreeram's [initial effort](<https://www.craft.me/s/WLFc7B9zgH4ncz>).

## Table of Contents

- [Year 2023](#2023)
- [Year 2022](#2022)
- [Year 2021](#2021)
- [Surveys](#Surveys)
- [Implementation references](#implementation-references)
    - [Bayesian Optimisation (BO) and DARTS](#Bayesian-Optimisation-(BO)-and-DARTS)

---

### 2023

| Title                                                                                                                                      | Venue |                               Code                               | Notes |
|:-------------------------------------------------------------------------------------------------------------------------------------------|:-----:|:----------------------------------------------------------------:|:-----:|
| [Stacking more layers differently: High-rank training through low-rank updates (ReLoRA)](https://arxiv.org/abs/2307.05695)                 | Arxiv |          [Github](https://github.com/Guitaricet/relora)          |       |
| [Neural architecture search for parameter-efficient fine-tuning of large pre-trained language models](https://arxiv.org/abs/2305.16597)    | Arxiv |                                -                                 |       |
| [DyLoRA: Parameter efficient tuning of pre-trained models using dynamic search-free low-rank adaptation](https://arxiv.org/abs/2210.07558) | Arxiv | [Github](https://github.com/huawei-noah/KD-NLP/tree/main/DyLoRA) |       |
| [LLM-Adapters: An adapter family for parameter-efficient fine-tuning of large language models](https://arxiv.org/abs/2304.01933)           | Arxiv |    [Github](https://github.com/AGI-Edgerunners/LLM-Adapters)     |       |
| [AutoPEFT: Automatic configuration search for parameter-efficient fine-tuning](https://arxiv.org/abs/2301.12132)                           | Arxiv |        [Github](https://github.com/cambridgeltl/autopeft)        |       |

---

### 2022

| Title                                                                                                                  | Venue |                        Code                         | Notes |
|:-----------------------------------------------------------------------------------------------------------------------|:-----:|:---------------------------------------------------:|:-----:|
| [SparseAdapter: An easy approach for improving the parameter-efficiency of adapters](https://arxiv.org/abs/2210.04284) | Arxiv | [Github](https://github.com/Shwai-He/SparseAdapter) |       |
| [Sparse structure search for parameter-efficient tuning](https://arxiv.org/abs/2206.07382)                             | Arxiv |     [Github](https://github.com/thunlp/S3Delta)     |       |
| [AdaMix: Mixture-of-adaptations for parameter-efficient model tuning](https://arxiv.org/abs/2205.12410)                | Arxiv |    [Github](https://github.com/microsoft/AdaMix)    |       |

---

### 2021
| Title                                                                                               | Venue |                                Code                                | Notes |
|:----------------------------------------------------------------------------------------------------|:-----:|:------------------------------------------------------------------:|:-----:|
| [Towards a unified view of parameter-efficient transfer learning](https://arxiv.org/abs/2110.04366) | Arxiv | [Github](https://github.com/jxhe/unify-parameter-efficient-tuning) |       |
| [LoRA: Low-rank adaptation of large language models](https://arxiv.org/abs/2106.09685)              | Arxiv |            [Github](https://github.com/microsoft/LoRA)             |       |

---

### Surveys

| Title | Venue |   Type   |   Code   |
|:------|:-----:|:--------:|:--------:|

### Implementation references

#### Bayesian Optimisation (BO) and DARTS

| Name                                                                                                                  | Notes |
|:----------------------------------------------------------------------------------------------------------------------|:------|
| [DARTS: Differentiable Architecture Search](https://arxiv.org/abs/1806.09055)                                         |       |
| [A Tutorial on Bayesian Optimization](https://arxiv.org/abs/1807.02811)                                               |       |
| [Optuna: A Next-generation Hyperparameter Optimization Framework](https://dl.acm.org/doi/pdf/10.1145/3292500.3330701) |       |

---

### Notes

